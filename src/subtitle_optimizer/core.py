from typing import List
import numpy as np
from pysrt import SubRipFile, SubRipItem
import pysrt
from subtitle_optimizer.utils import call_llm_api, detect_language, format_merged_text
from subtitle_optimizer.exceptions import LanguageMismatchError
import whisper

class SubtitleOptimizer:
    def __init__(self, api_key=None):
        self.api_key = api_key
        # åˆå§‹åŒ–æœ¬åœ°Whisperå¤§æ¨¡å‹ï¼ˆæ¨èä½¿ç”¨large-v3ç‰ˆæœ¬ï¼‰[4](@ref)
        self.whisper_model = whisper.load_model("large-v3") 


    ####################ä¿®æ”¹å­—å¹•ä¸­çš„å•è¯çš„é”™è¯¯æ‹¼å†™######################
    def correct_spelling(self, subs: SubRipFile, audio_path: str) -> SubRipFile:
        """
        é€šè¿‡Whisperè¯­éŸ³è¯†åˆ«å’ŒLLM APIä¿®æ­£å­—å¹•æ‹¼å†™é”™è¯¯
        :param subs: å­—å¹•æ–‡ä»¶å¯¹è±¡
        :param audio_path: å¯¹åº”çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :return: ä¿®æ­£åçš„å­—å¹•æ–‡ä»¶å¯¹è±¡
        """
        # åŠ è½½éŸ³é¢‘å¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„[4](@ref)
        audio = whisper.load_audio(audio_path)
        
        for sub in subs:
            # æå–å­—å¹•æ—¶é—´æ®µï¼ˆæ¯«ç§’è½¬æ¢ä¸ºç§’ï¼‰
            start_sec = sub.start.ordinal / 1000
            end_sec = sub.end.ordinal / 1000
            
            # åˆ‡å‰²éŸ³é¢‘ç‰‡æ®µ[4](@ref)
            segment = self._extract_audio_segment(audio, start_sec, end_sec)
            
            # è¯­éŸ³è¯†åˆ«ï¼ˆä½¿ç”¨Whisperçš„decodeæ¨¡å¼æå‡ç²¾åº¦ï¼‰[4](@ref)
            result = self.whisper_model.transcribe(
                segment,
                language=detect_language(sub.text),  # å¤ç”¨ç°æœ‰è¯­è¨€æ£€æµ‹
                word_timestamps=False
            )
            whisper_text = result["text"].strip()
            
            # ç”Ÿæˆæ‹¼å†™ä¿®æ­£æŒ‡ä»¤[6](@ref)
            prompt = f"""è¯·ä¿®æ­£ä»¥ä¸‹å­—å¹•æ–‡æœ¬çš„æ‹¼å†™é”™è¯¯ï¼Œä¿æŒæ—¶é—´è½´åŒæ­¥ï¼š
            åŸæ–‡æœ¬ï¼š{sub.text}
            å‚è€ƒè½¬å†™ï¼š{whisper_text}
            è¦æ±‚ï¼š
            1. ä¿®æ­£æ˜æ˜¾çš„æ‹¼å†™/è¯­æ³•é”™è¯¯
            2. ä¿ç•™ä¸“ä¸šæœ¯è¯­
            3. è¾“å‡ºæ ¼å¼ï¼šä»…è¿”å›ä¿®æ­£æ–‡æœ¬"""
            
            # è°ƒç”¨LLM APIï¼ˆæ”¯æŒå¤šå¹³å°è°ƒç”¨ï¼‰[6,8](@ref)
            corrected = call_llm_api(
                prompt, 
                api_key=self.api_key,
                model="qwen-max"  # æ¨èä½¿ç”¨æœ€æ–°æ¨¡å‹
            )
            sub.text = corrected.split("\n")[-1]  # å–æœ€åä¸€è¡Œé˜²æ­¢é™„åŠ è¯´æ˜
        
        return subs

    def _extract_audio_segment(self, audio, start_sec: float, end_sec: float):
        """éŸ³é¢‘åˆ‡å‰²å·¥å…·ï¼ˆéœ€å®‰è£…pydubï¼‰"""
        from pydub import AudioSegment
        return audio[start_sec*1000:end_sec*1000]

    def correct_and_save(self, srt_path: str, audio_path: str):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        subs = pysrt.open(srt_path)
        corrected = self.correct_spelling(subs, audio_path)
        corrected.save(srt_path, encoding='utf-8')
        print(f"å·²å®Œæˆæ‹¼å†™ä¿®æ­£å¹¶è¦†ç›–ä¿å­˜ï¼š{srt_path}")
    


    ####################å°†å­—å¹•ä¸­çš„çŸ­å¥åˆå¹¶ä¸ºé•¿å¥######################

    def merge_srt(self, input_srt_file: str, output_srt_file: str = None) -> None:
        """å¤„ç†SRTæ–‡ä»¶åˆå¹¶å¹¶ä¿å­˜"""
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(input_srt_file, 'r', encoding='utf-8') as f:
            subs = srt.parse(f.read())
        
        # è°ƒç”¨åŸæœ‰åˆå¹¶é€»è¾‘
        merged_subs = self._merge_srt(subs)
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        save_path = output_srt_file if output_srt_file else input_srt_file
        
        # å†™å…¥å¤„ç†ç»“æœ
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(srt.compose(merged_subs))
        
        print(f"å¤„ç†å®Œæˆï¼Œä¿å­˜è·¯å¾„ï¼š{save_path}")


    def _merge_srt(self, subs: SubRipFile,max_merge_lines=999) -> SubRipFile:
        merged_subs = []
        i = 0
        
        while i < len(subs):
            current_group = [subs[i]]
            merged_text = subs[i].text
            next_candidate = i + 1
            
            print(f"\nå¼€å§‹å¤„ç†å­—å¹•ç»„ï¼Œèµ·å§‹ç´¢å¼•: {i}")
            print(f"åˆå§‹åˆå¹¶ç»„: {[sub.text for sub in current_group]}")
            print(f"åˆå§‹åˆå¹¶æ–‡æœ¬: '{merged_text}'")

            while True:
                if next_candidate >= len(subs) or len(current_group) >= self.max_merge_lines:
                    print(f"ç»ˆæ­¢åˆå¹¶ - å‰©ä½™å­—å¹•: {len(subs)-next_candidate}, å½“å‰åˆå¹¶è¡Œæ•°: {len(current_group)}")
                    break
                
                next_sub = subs[next_candidate]
                next_next_sub = subs[next_candidate + 1] if next_candidate + 1 < len(subs) else None
                
                try:
                    args = [merged_text, next_sub.text]
                    if next_next_sub:
                        args.append(next_next_sub.text)
                    
                    # æ‰“å°åˆå¹¶åˆ¤æ–­å‚æ•°
                    print(f"\nå°è¯•åˆå¹¶ {len(current_group)} è¡Œåˆ° {len(current_group)+1} è¡Œ")
                    print(f"å½“å‰åˆå¹¶æ–‡æœ¬: '{args[0]}'")
                    print(f"ä¸‹ä¸€è¡Œæ–‡æœ¬:  '{args[1]}'")
                    print(f"ç¬¬ä¸‰è¡Œå­˜åœ¨:   {args[2] if len(args)>2 else 'æ— '}")

                    if self._should_merge(*args):
                        current_group.append(next_sub)
                        merged_text = self._format_text(current_group)
                        next_candidate += 1
                        print(f"âœ… åˆå¹¶æˆåŠŸï¼å½“å‰åˆå¹¶è¡Œæ•°: {len(current_group)}")
                        print(f"æ›´æ–°ååˆå¹¶æ–‡æœ¬: '{merged_text}'")
                    else:
                        print(f"âŒ åˆå¹¶ç»ˆæ­¢ {len(current_group)} è¡Œ")
                        break
                except LanguageMismatchError as e:
                    print(f"ğŸš« è¯­è¨€ä¸åŒ¹é…å¼‚å¸¸: {str(e)}")
                    break

            new_sub = self._create_merged_sub(current_group, merged_text)
            merged_subs.append(new_sub)
            print(f"\nç”Ÿæˆåˆå¹¶æ¡ç›®: {new_sub.start} -> {new_sub.end}")
            print(f"æœ€ç»ˆåˆå¹¶æ–‡æœ¬: '{new_sub.text}'\n")
            
            i = next_candidate
        
        print(f"\nåˆå¹¶å®Œæˆï¼Œæ€»æ¡ç›®æ•°: {len(merged_subs)}")
        return SubRipFile(items=merged_subs)
        
    def _format_text(self, group):
        return format_merged_text([sub.text for sub in group])
    
    def _create_merged_sub(self, group, text):
        return SubRipItem(
            start=group[0].start,
            end=group[-1].end,
            text=text
        )
    
    def _should_merge(self, text1: str, text2: str, text3: str=None) -> bool:
        print(f"\næ‰§è¡Œåˆå¹¶åˆ¤æ–­ _should_merge:")
        print(f"text1 ({len(text1)} chars): '{text1}'")
        print(f"text2 ({len(text2)} chars): '{text2}'")
        print(f"text3 ({len(text3) if text3 else 'None'}): '{text3 if text3 else 'æ— '}'")

        if detect_language(text1) != detect_language(text2):
            print("è¯­è¨€ä¸åŒ¹é…è§¦å‘å¼‚å¸¸")
            raise LanguageMismatchError("Language mismatch")

        # æ‰“å°è¯­è¨€æ£€æµ‹ç»“æœ
        lang1 = detect_language(text1)
        lang2 = detect_language(text2)
        lang3 = detect_language(text3) if text3 else None
        print(f"è¯­è¨€æ£€æµ‹ç»“æœ: text1={lang1}, text2={lang2}, text3={lang3}")

        if lang1 != lang2 or lang2 != lang3:
            print(f"è¯­è¨€ä¸åŒ¹é…: {lang1} != {lang2} != {lang3} text1={text1} text2={text2} text3={text3}")
            return False

        prompt = ""
        if text3 is not None and lang1 == lang2 and lang2 == lang3:
            prompt = f"åœ¨ä¸€ä¸ªæ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„å­—å¹•æ–‡ä»¶ä¸­ï¼Œç¬¬ä¸‰è¡Œå­—å¹•æ˜¯â€œ{text3}â€çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€è¡Œå­—å¹•å’Œç¬¬äºŒè¡Œæ˜¯å¦å¯èƒ½æ˜¯å±äºåŒä¸€å¥è¯?å¯èƒ½å±äºçš„å›ç­”æˆ‘â€œYâ€ï¼Œå¯èƒ½ä¸å±äºå›ç­”æˆ‘â€œNâ€ï¼Œä¸è¦å›ç­”ä»»ä½•ä¿¡æ¯ã€‚\nç¬¬ä¸€è¡Œ: {text1}\nç¬¬äºŒè¡Œ: {text2}"
        else:
            prompt = f"åœ¨ä¸€ä¸ªæ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„å­—å¹•æ–‡ä»¶ä¸­ï¼Œåœ¨æ²¡æœ‰ç¬¬ä¸‰è¡Œå­—å¹•çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€è¡Œå­—å¹•å’Œç¬¬äºŒè¡Œæ˜¯å¦å¯èƒ½æ˜¯å±äºåŒä¸€å¥è¯?å¯èƒ½å±äºçš„å›ç­”æˆ‘â€œYâ€ï¼Œå¯èƒ½ä¸å±äºå›ç­”æˆ‘â€œNâ€ï¼Œä¸è¦å›ç­”ä»»ä½•ä¿¡æ¯ã€‚\nç¬¬ä¸€è¡Œ: {text1}\nç¬¬äºŒè¡Œ: {text2}"
        
        print(f"å‘é€ç»™LLMçš„æç¤º:\n{prompt}")
        response = call_llm_api(prompt, api_key=self.api_key)
        print(f"LLMå“åº”åŸå§‹ç»“æœ: '{response}'")
        
        result = 'Y' in response
        print(f"åˆå¹¶åˆ¤æ–­æœ€ç»ˆç»“æœ: {'Y' if result else 'N'}")
        return result


    ####################å°†å­—å¹•ä¸­çš„é•¿å¥æ‰åˆ†ä¸ºçŸ­å¥######################


    def split_long_lines(self, input_srt_file: str, output_srt_file: str = None, max_line_count=33) -> None:
        """
        ä¼˜åŒ–å­—å¹•æ–‡ä»¶ï¼šæ‹†åˆ†è¶…é•¿è¡Œå¹¶é‡æ–°è®¡ç®—æ—¶é—´è½´
        :param input_srt_file: è¾“å…¥SRTæ–‡ä»¶è·¯å¾„
        :param output_srt_file: è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„ï¼ˆç©ºåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
        """
        subs = pysrt.open(input_srt_file)
        audio = whisper.load_audio(input_srt_file.replace('.srt', '.mp4'))  # å‡è®¾è§†é¢‘æ–‡ä»¶ä¸SRTåŒåâ€Œ:ml-citation{ref="1" data="citationList"}
        
        new_subs = SubRipFile()
        for idx, sub in enumerate(subs):
            if len(sub.text) <= max_line_count:
                new_subs.append(sub)
                continue
                
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œè¯­ä¹‰æ‹†åˆ†â€Œ:ml-citation{ref="2" data="citationList"}
            prompt = f"å°†ä»¥ä¸‹ä¸€è¡Œé•¿å­—å¹•å†…å®¹æŒ‰è‹±æ–‡è¯­ä¹‰æ‹†åˆ†ä¸ºå¤šè¡Œå­—å¹•æ˜¾ç¤ºï¼Œä¿æŒåŸå¥å†…å®¹çš„ä¸å˜ï¼Œä¸è¦è¿”å›è¯­å¥å¤–çš„å…¶ä»–ä»»ä½•å¤šä½™çš„å†…å®¹ï¼Œå°†æ‹†åˆ†å‡ºæ¥çš„å¤šè¡Œå­—å¹•çš„æ¯ä¸€è¡Œç”¨æ¢è¡Œéš”å¼€ï¼š\n{sub.text}"
            response = call_llm_api(prompt, self.api_key, model="qwen-max")
            split_lines = [line.strip() for line in response.split('\n') if line.strip()]
            
            # åˆ†å‰²æ—¶é—´æ®µå¹¶ç”Ÿæˆæ–°å­—å¹•é¡¹â€Œ:ml-citation{ref="1" data="citationList"}
            time_segments = self._split_time_segment(sub, len(split_lines), audio)
            for i, (text, (start, end)) in enumerate(zip(split_lines, time_segments)):
                new_item = SubRipItem(
                    index=len(new_subs)+1,
                    start=pysrt.SubRipTime(milliseconds=start),
                    end=pysrt.SubRipTime(milliseconds=end),
                    text=text
                )
                new_subs.append(new_item)
        
        save_path = output_srt_file or input_srt_file
        new_subs.save(save_path, encoding='utf-8')
    
    def _split_time_segment(self, sub: SubRipItem, split_count: int, audio: np.ndarray) -> List[tuple]:
        """
        ä½¿ç”¨Whisperç²¾ç¡®åˆ†å‰²æ—¶é—´æ®µâ€Œ:ml-citation{ref="1" data="citationList"}
        """
        start_ms = sub.start.ordinal
        end_ms = sub.end.ordinal
        segment_duration = (end_ms - start_ms) / split_count
        
        # å½“åˆ†å‰²æ¬¡æ•°è¾ƒå¤šæ—¶é‡‡ç”¨Whisperç²¾ç¡®è¯†åˆ«â€Œ:ml-citation{ref="1" data="citationList"}
        if split_count > 2:
            result = self.whisper_model.transcribe(
                audio[start_ms//1000:end_ms//1000],
                word_timestamps=True
            )
            return self._align_whisper_timestamps(result, split_count)
        
        # ç®€å•å¹³å‡åˆ†é…æ—¶é—´
        return [(start_ms + i*segment_duration, 
                start_ms + (i+1)*segment_duration) 
                for i in range(split_count)]

    def _align_whisper_timestamps(self, whisper_result, expected_splits: int) -> List[tuple]:
        """
        å¯¹é½Whisperè¯†åˆ«çš„æ—¶é—´æˆ³ä¸æ‹†åˆ†åçš„æ–‡æœ¬â€Œ:ml-citation{ref="1" data="citationList"}
        """
        # å®ç°é€»è¾‘ï¼šæ ¹æ®è¯­éŸ³åœé¡¿å’Œè¯è¾¹ç•Œè°ƒæ•´æ—¶é—´åˆ†å‰²ç‚¹
        # ï¼ˆæ­¤å¤„éœ€è¦æ ¹æ®å®é™…è¯­éŸ³ç‰¹å¾è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼‰
        return [(word['start']*1000, word['end']*1000) 
                for word in whisper_result['words'][:expected_splits]]
import asyncio
import json
import os
import re
import traceback
from types import SimpleNamespace
from typing import List
import edge_tts
import numpy as np
from pysrt import SubRipFile, SubRipItem
import pysrt
from subtitle_optimizer.utils import detect_encoding,merge_short_subs,time_stretch_audio,call_llm_api, detect_language, format_merged_text
from subtitle_optimizer.exceptions import LanguageMismatchError
import whisper
from pydub import AudioSegment
import torch
from typing import Union, List, Tuple
import time
from functools import wraps
from moviepy import VideoFileClip,AudioFileClip

def retry_on_permission_error(max_retries=3, delay=0.5):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except PermissionError as e:
                    retries += 1
                    if retries >= max_retries:
                        raise
                    print(f"Retry {retries}/{max_retries} due to {e}")
                    time.sleep(delay)
        return wrapper
    return decorator

# åœ¨åˆ é™¤å’Œé‡å‘½åæ“ä½œå¤„æ·»åŠ è£…é¥°å™¨
@retry_on_permission_error()
def safe_remove(filepath):
    if os.path.exists(filepath):
        os.remove(filepath)

@retry_on_permission_error()
def safe_rename(src, dst):
    os.rename(src, dst)

class SubtitleOptimizer:
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ– SubtitleOptimizer ç±»çš„å®ä¾‹ã€‚

        å‚æ•°:
            api_key (str, å¯é€‰): ç”¨äºè°ƒç”¨ LLM API çš„ API å¯†é’¥ã€‚é»˜è®¤ä¸º Noneã€‚
        """
        self.api_key = api_key
        # åˆå§‹åŒ–æœ¬åœ°Whisperå¤§æ¨¡å‹
        # æ³¨é‡Šæ‰çš„ä»£ç è¡¨ç¤ºä½¿ç”¨ "large-v3" ç‰ˆæœ¬çš„ Whisper æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šå¸¸å…·æœ‰æ›´é«˜çš„ç²¾åº¦ï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚
        # self.whisper_model = whisper.load_model("large-v3")
        # ä½¿ç”¨ "medium.en" ç‰ˆæœ¬çš„ Whisper æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“ä¸ºè‹±è¯­è®¾è®¡ï¼Œåœ¨ç²¾åº¦å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è¾ƒå¥½çš„å¹³è¡¡ã€‚
        self.whisper_model = whisper.load_model("medium.en")

    ###################è§†é¢‘å˜é€Ÿ######################
    def adjust_video_speed(self, video_path: str, speed_factor: float = 0.7) -> None:
        """
        è°ƒæ•´è§†é¢‘æ’­æ”¾é€Ÿç‡å¹¶è¦†ç›–åŸæ–‡ä»¶
        
        å‚æ•°:
            video_path (str): MP4è§†é¢‘æ–‡ä»¶è·¯å¾„
            speed_factor (float): é€Ÿç‡å€æ•°ï¼ˆé»˜è®¤0.7å€é™é€Ÿï¼‰
        
        ç¤ºä¾‹:
            obj = SubtitleOptimizer()
            obj.adjust_video_speed("test.mp4", 0.5)  # é™é€Ÿè‡³0.5å€
        """
        if speed_factor==1:
            print("è§†é¢‘é€Ÿç‡å·²ä¸ºæ­£å¸¸é€Ÿç‡ï¼Œæ— éœ€è°ƒæ•´")
            return
        try:
            video_path = os.path.abspath(video_path)

            voice_wav = time_stretch_audio(video_path, speed_factor)
            audio = AudioFileClip(voice_wav)  # æ–°å¢ä»£ç 

            if os.path.exists(video_path) is False:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æŒ‡å®šçš„è§†é¢‘æ–‡ä»¶ï¼š{video_path}")

            # åŠ è½½è§†é¢‘
            video = VideoFileClip(video_path)
            # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_path = video_path.replace(".mp4", "_temp.mp4")
            print(f"è°ƒæ•´è§†é¢‘é€Ÿç‡è‡³ {speed_factor} å€ä¸­... ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼š{temp_path}")
            
            # è§†é¢‘å¤„ç†
            adjusted_video = video.with_speed_scaled(speed_factor).with_audio(audio)
            temp_path = video_path.replace(".mp4", "_temp.mp4")
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆæ–°å¢remove_tempå‚æ•°ï¼‰
            adjusted_video.write_videofile(
                temp_path,
                codec='libx264',
                audio_codec='aac',
                threads=4,
                logger=None,
                remove_temp=True  # å¼ºåˆ¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            )
            
            # æ˜¾å¼é‡Šæ”¾èµ„æºï¼ˆå…³é”®ä¿®å¤ç‚¹ï¼‰
            for obj in [video, audio, adjusted_video]:
                if obj: obj.close()
            
            # è¦†ç›–åŸæ–‡ä»¶ï¼ˆæ–°å¢é‡è¯•æœºåˆ¶ï¼‰
            retry_count = 0
            while retry_count < 3:
                try:
                    os.replace(temp_path, video_path)
                    break
                except PermissionError:
                    time.sleep(0.5)  # ç­‰å¾…500msé‡è¯•
                    retry_count +=1
            
            print(f"è¦†ç›–å®Œæˆ: {video_path}")

        except Exception as e:
            # æœ€ç»ˆèµ„æºæ¸…ç†ï¼ˆæ–°å¢ï¼‰
            for obj in [video, audio, adjusted_video]:
                if obj and hasattr(obj, 'close'): obj.close()
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.remove(temp_path)
            if os.path.exists(voice_wav):
                os.remove(voice_wav)
            


    ####################å°†srtè½¬æ¢ä¸ºè¯­éŸ³######################

    def edge_tts_voice(self,args):
        async def _text_to_speech():
            # è·å–å‚æ•°å†…å®¹
            content = args.content
            save_path = args.save_path
            language = args.language
            voice = args.voice
            rate = getattr(args, 'rate', '+0%')  # æ·»åŠ è¯­é€Ÿå‚æ•°ï¼Œé»˜è®¤ä¸ºæ­£å¸¸è¯­é€Ÿ

            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not content:
                raise ValueError("Content cannot be empty")

            # è®¾ç½®é»˜è®¤è¯­è¨€ä¸ºè‹±æ–‡
            if language is None:
                language = "en"

            # è®¾ç½®é»˜è®¤è¯­éŸ³
            if voice is None:
                if language == "en":
                    voice = "en-US-MichelleNeural"  # é»˜è®¤è‹±æ–‡
                elif language == "en-child":
                    voice = "en-US-AnaNeural"  # é»˜è®¤å„¿ç«¥è‹±æ–‡
                elif language == "zh-cn":
                    voice = "zh-CN-XiaoxiaoNeural"  # é»˜è®¤ä¸­æ–‡
                elif language == "zh-tw":
                    voice = "zh-TW-HsiaoChenNeural"  # é»˜è®¤å°æ¹¾ä¸­æ–‡
                else:
                    raise ValueError(f"Unsupported language: {language}")
                
            communicate = edge_tts.Communicate(
                content, 
                voice, 
                rate=rate
            )
            await communicate.save(save_path)
            print(f"Audio saved successfully at {save_path}")

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(_text_to_speech())
        finally:
            loop.close()

    def generate_voice_from_srt(self, srt_path: str):
        """å¤„ç†SRTæ–‡ä»¶ç”ŸæˆåŒè¯­è¯­éŸ³å¹¶è°ƒæ•´æ—¶é•¿"""
        # åˆ›å»ºè¯­éŸ³æ–‡ä»¶å¤¹
        srt_path = os.path.abspath(srt_path)
        voice_dir = os.path.join(os.path.dirname(srt_path) , f"{os.path.basename(srt_path).split(".")[0]}-voice")
        if not os.path.exists(voice_dir):
            os.mkdir(voice_dir)
        else:
            raise FileExistsError(f"ç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼š{voice_dir}")
        
        # å¤„ç†SRTæ–‡ä»¶
        subs = pysrt.open(srt_path, encoding='utf-8')
        cn_audio_files = []
        
        for idx, sub in enumerate(subs, start=1):
            # åˆ†å‰²å­—å¹•å†…å®¹
            lines = sub.text.split('\n')
            
            # åˆå¹¶ä¸­è‹±æ–‡å­—å¹•
            merged = ['', '']
            current_lang = 0  # 0-ä¸­æ–‡ 1-è‹±æ–‡
            for line in lines:
                has_chinese = bool(re.search(r'[\u4e00-\u9fff]', line))
                target = 0 if has_chinese else 1
                merged[target] += ' ' + line.strip()
            
            print(f"\nå¤„ç†å­—å¹•ï¼š{idx} | ä¸­æ–‡ï¼š{merged[0]} | è‹±æ–‡ï¼š{merged[1]}")

            # ç”Ÿæˆè¯­éŸ³æ–‡ä»¶
            for i, text in enumerate(merged):
                if not text.strip():
                    continue
                
                prefix = 'cn' if i == 0 else 'en'
                filename = os.path.join(voice_dir , f"{prefix}-{idx:04d}.mp3") 
                if prefix == 'cn':
                    args = SimpleNamespace(
                        content=text,
                        save_path=str(filename),
                        language='zh-cn',
                        rate='+33%',
                        voice=None
                    )
                else:
                    args = SimpleNamespace(
                        content=text,
                        save_path=str(filename),
                        language='en',
                        rate='+0%',
                        voice=None
                    )
                
                # è°ƒç”¨è¯­éŸ³ç”Ÿæˆï¼ˆéœ€å¤„ç†å¼‚æ­¥ï¼‰
                self.edge_tts_voice(args)


                if prefix == 'cn':
                    cn_audio_files.append((filename, sub))
        
        # è°ƒæ•´ä¸­æ–‡è¯­éŸ³æ—¶é•¿ï¼ˆä¿®å¤å˜é€Ÿå¤±æ•ˆé—®é¢˜ï¼‰
        for filepath, sub in cn_audio_files:
            audio = AudioSegment.from_file(filepath)
            original_duration = len(audio) / 1000  # è½¬æ¢ä¸ºç§’
            
            # è®¡ç®—ç›®æ ‡æ—¶é•¿ï¼ˆæ¯«ç§’è½¬ç§’ï¼‰
            target_duration = (sub.end.ordinal - sub.start.ordinal) / 1000.0
            
            # æœ‰æ•ˆé€Ÿåº¦èŒƒå›´ï¼ˆåŸºäºè¯­éŸ³æ¸…æ™°åº¦ç ”ç©¶ï¼‰
            min_speed = 0.7  # æœ€ä½è¯­é€Ÿï¼ˆä½äº0.7xä¼šå¯¼è‡´ä¸¥é‡å¤±çœŸï¼‰
            max_speed = 2.0  # æœ€é«˜è¯­é€Ÿï¼ˆé«˜äº2.0xä¼šä¸¢å¤±è¯­éŸ³ç‰¹å¾ï¼‰
            
            if abs(original_duration - target_duration) > 0.1:  # è°ƒæ•´å®¹å·®é˜ˆå€¼è‡³0.1ç§’
                print(f"è°ƒæ•´è¯­éŸ³æ—¶é•¿ï¼š{filepath} | å½“å‰={original_duration:.2f}s | ç›®æ ‡={target_duration:.2f}s")
                
                # ä¿®å¤1ï¼šä½¿ç”¨æ­£ç¡®çš„é€Ÿåº¦å› å­è®¡ç®—ï¼ˆå½“éœ€è¦å‡é€Ÿæ—¶å–å€’æ•°ï¼‰
                speed_factor = original_duration / target_duration
                
                # ä¿®å¤2ï¼šé€Ÿåº¦é™åˆ¶é€»è¾‘é‡æ„
                if speed_factor > 1:  # åŠ é€Ÿæ¨¡å¼
                    speed_factor = min(max_speed, speed_factor)
                    processing_method = "åŠ é€Ÿ"
                else:  # å‡é€Ÿæ¨¡å¼
                    speed_factor = max(min_speed, 1/speed_factor)
                    processing_method = "å‡é€Ÿ"
                
                print(f"é€Ÿåº¦ç³»æ•°ï¼š{speed_factor:.2f}x ({processing_method})")
                
                # ä¿®å¤3ï¼šä½¿ç”¨speedupæ–¹æ³•å¹¶ä¿æŒéŸ³é«˜
                adjusted_audio = audio.speedup(
                    playback_speed=speed_factor,
                    chunk_size=150,  # å‡å°å—å¤§å°æå‡ç²¾åº¦
                    crossfade=25      # æ·»åŠ äº¤å‰æ·¡åŒ–
                )
                
                # æ–°å¢ï¼šåŠ¨æ€æ»¤æ³¢å¤„ç†ï¼ˆæ ¹æ®é€Ÿåº¦è°ƒæ•´æˆªæ­¢é¢‘ç‡ï¼‰
                if speed_factor > 1.5:
                    adjusted_audio = adjusted_audio.low_pass_filter(6000)  # æŠ‘åˆ¶é«˜é¢‘å™ªå£°
                elif speed_factor < 0.7:
                    adjusted_audio = adjusted_audio.high_pass_filter(200)  # å¢å¼ºä½é¢‘
                    
                # æ–°å¢ï¼šæŒ¯å¹…å½’ä¸€åŒ–é˜²æ­¢å‰Šæ³¢
                adjusted_audio = adjusted_audio.normalize()
                
                # ä¿®å¤4ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶é¿å…è¦†ç›–é—®é¢˜
                temp_path = f"{filepath}.tmp"
                adjusted_audio.export(temp_path, format="mp3", bitrate="192k")  # å›ºå®šæ¯”ç‰¹ç‡
                
                # åŸå­æ›¿æ¢æ–‡ä»¶ï¼ˆWindowséœ€å¤„ç†æ–‡ä»¶å ç”¨é—®é¢˜ï¼‰
                if os.path.exists(filepath):
                    safe_remove(filepath)
                safe_rename(temp_path, filepath)
            else:
                print(f"è·³è¿‡è°ƒæ•´ï¼ˆå·®å€¼{abs(original_duration - target_duration):.2f}s < 0.1sï¼‰ï¼š{filepath}")


    ####################é€šè¿‡txtç”ŸæˆSRTæ–‡ä»¶######################
    @staticmethod
    def _split_sentences(content):
        pattern = r'([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€¦\u2026.!?;:])[\s]*'
        result = re.split(pattern, content)
        
        sentences = []
        # åˆæ­¥åˆå¹¶å¥å­ä¸æ ‡ç‚¹
        for i in range(0, len(result), 2):
            combined = (result[i] + (result[i+1] if i+1 < len(result) else '')).strip()
            if combined:
                sentences.append(combined)
        
        # å¤„ç†å•ç‹¬æ ‡ç‚¹åˆå¹¶åˆ°å‰ä¸€å¥
        merged = []
        for sentence in sentences:
            # åˆ¤æ–­å½“å‰å¥å­æ˜¯å¦ä»…ç”±æ ‡ç‚¹æ„æˆ
            if re.fullmatch(r'^[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€¦.!?;:]+$', sentence):
                # è‹¥å‰ä¸€å¥å­˜åœ¨ä¸”éæ ‡ç‚¹ï¼Œåˆ™åˆå¹¶
                if merged and not re.fullmatch(r'^[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€¦.!?;:]+$', merged[-1]):
                    merged[-1] += sentence
                else:
                    merged.append(sentence)
            else:
                merged.append(sentence)
        return merged


    def _generate_srt_from_segments(self, txt_path: str, mp4_path: str, segments_path: str = None):
        """å¤„ç†å•ä¸ªæ–‡ä»¶å¯¹ï¼Œæ”¯æŒå¯é€‰çš„åˆ†æ®µæ—¶é—´æˆ³æ–‡ä»¶"""
        srt_path = os.path.splitext(mp4_path)[0] + ".srt"
        if os.path.exists(srt_path):
            print(f"SRTæ–‡ä»¶å·²å­˜åœ¨ï¼š{srt_path} è·³è¿‡å¤„ç†\n")
            return
        # æ—¶é—´æˆ³æ¥æºåˆ¤æ–­
        if os.path.exists(segments_path):
            # ä»æ–‡ä»¶åŠ è½½é¢„ç”Ÿæˆçš„æ—¶é—´æˆ³
            try:
                with open(segments_path, 'r', encoding='utf-8') as f:
                    segments = json.load(f)  # åŠ è½½JSONæ•°æ®
            except json.JSONDecodeError:
                print("é”™è¯¯ï¼šæ–‡ä»¶å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                traceback.print_exc()
            except PermissionError:
                print("é”™è¯¯ï¼šæ— æƒé™è¯»å–æ–‡ä»¶")
                traceback.print_exc()
            except Exception as e:
                print(f"æœªçŸ¥é”™è¯¯ï¼š{e}")
                traceback.print_exc()
        else:
            print(f"â³ æ­£åœ¨æå– {mp4_path} ä¸­çš„æ—¶é—´æˆ³... segments_path ä¸å­˜åœ¨:{segments_path}")
            # é€šè¿‡Whisperç”Ÿæˆæ—¶é—´æˆ³
            result = self.whisper_model.transcribe(
                mp4_path,
                word_timestamps=True,
                language="en",
                task='transcribe',
                fp16=torch.cuda.is_available()
            )
            segments = result["segments"]
        print(f"âœ… æå–åˆ° {len(segments)} ä¸ªæ—¶é—´æˆ³æ®µ")
        
        # è¯»å–æ–‡æœ¬å†…å®¹ï¼ˆé€»è¾‘ä¸å˜ï¼‰
        content = ""
        if txt_path is not None:
            with open(txt_path, "r", encoding="utf-8") as f:
                content = f.read()
        elif segments is not None:
            for segment in segments:
                content+= segment.get("text", "")
        sentences = SubtitleOptimizer._split_sentences(content) # æŒ‰ç…§æ ‡ç‚¹ç¬¦å·åˆ’åˆ†å¥å­
        print(f"âœ… æå–åˆ° {len(sentences)} ä¸ªå¥å­")
        print(f"å¥å­ï¼š{sentences}\n")
        
        # ç”ŸæˆSRTå†…å®¹
        all_words = [word for segment in segments for word in segment.get("words", [])]
        srt_content = []
        current_word_idx = 0

        for sentence in sentences:
            # æ–°å¢æ ‡ç‚¹è§„èŒƒåŒ–å¤„ç†ï¼ˆè§£å†³ä¸­è‹±æ–‡ç¬¦å·å·®å¼‚ï¼‰
            normalized_sentence = re.sub(r'[ï¼Œ,]', ',', sentence)  # ç»Ÿä¸€ä¸­æ–‡é€—å·ä¸ºè‹±æ–‡
            sentence_words = normalized_sentence.split()
            
            matched = False
            max_retry = 13  # æœ€å¤§åˆå¹¶å°è¯•æ¬¡æ•°
            
            # æ‰©å±•åŒ¹é…èŒƒå›´ï¼Œè€ƒè™‘å•è¯åˆå¹¶å¯èƒ½æ€§
            for i in range(current_word_idx, min(len(all_words), current_word_idx + 200)):
                # å°è¯•åˆå¹¶1åˆ°13ä¸ªå•è¯çš„ç»„åˆ
                for merge_count in range(1, max_retry + 1):
                    if i + merge_count > len(all_words):
                        break
                    
                    # ç”Ÿæˆåˆå¹¶åçš„å•è¯åŠæ—¶é—´æˆ³
                    merged_word = ''.join([all_words[idx]['word'].strip() for idx in range(i, i + merge_count)])
                    merged_start = all_words[i]['start']
                    merged_end = all_words[i + merge_count - 1]['end']
                    
                    # è§„èŒƒåŒ–æ¯”è¾ƒï¼ˆå¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼ï¼‰
                    if merged_word.lower() == sentence_words[0].lower():
                        # éªŒè¯åç»­å•è¯æ˜¯å¦è¿ç»­åŒ¹é…
                        full_match = True
                        word_ptr = i + merge_count
                        
                        for word_idx in range(1, len(sentence_words)):
                            # ç»§ç»­å°è¯•åˆå¹¶åç»­å•è¯
                            found = False
                            for cnt in range(1, max_retry + 1):
                                if word_ptr + cnt > len(all_words):
                                    break
                                
                                next_merged = ''.join([all_words[idx]['word'].strip() 
                                                    for idx in range(word_ptr, word_ptr + cnt)])
                                
                                if next_merged.lower() == sentence_words[word_idx].lower():
                                    word_ptr += cnt
                                    merged_end = all_words[word_ptr - 1]['end']
                                    found = True
                                    break
                            
                            if not found:
                                full_match = False
                                break
                        
                        if full_match:
                            srt_content.append(
                                f"{len(srt_content)+1}\n"
                                f"{self._format_timestamp(merged_start)} --> {self._format_timestamp(merged_end)}\n"
                                f"{sentence}\n"
                            )
                            current_word_idx = word_ptr
                            matched = True
                            break
                    
                    if matched:
                        break
                if matched:
                    break

            if not matched:
                # å¢å¼ºé”™è¯¯ä¿¡æ¯å¯è¯»æ€§
                expected = '|'.join(sentence_words)
                actual = '|'.join([w['word'] for w in all_words[current_word_idx:current_word_idx+20]])
                raise ValueError(
                    f"æ— æ³•åŒ¹é…å¥å­ï¼š'{sentence}'\n"
                    f"é¢„æœŸå•è¯åºåˆ—ï¼š{expected}\n"
                    f"å®é™…åç»­å•è¯ï¼š{actual}"
                )


        # ä¿å­˜SRTæ–‡ä»¶
        with open(srt_path, "w", encoding="utf-8") as f:
            f.write("\n".join(srt_content))
            print(f"âœ… SRTæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{srt_path}")
        
        merge_short_subs(srt_path)  # åˆå¹¶çŸ­å¥
        print(f"åˆå¹¶åçš„srtæ–‡ä»¶ï¼š")
        for line in open(srt_path, 'r', encoding='utf-8'):
            print(line)

    def generate_srt_from_directory(self, folder_path: str):
        """å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶å¯¹"""
        mp4_files = [f for f in os.listdir(folder_path) if f.endswith(".mp4")]
        for mp4_file in mp4_files:
            try:
                base_name = os.path.splitext(mp4_file)[0]
                # ç”Ÿæˆä¸‰ç§å¯èƒ½çš„ç›¸å…³æ–‡ä»¶è·¯å¾„
                txt_path = os.path.join(folder_path, f"{base_name}.txt")
                segments_path = os.path.join(folder_path, f"{base_name}_segments.txt")
                mp4_path = os.path.join(folder_path, mp4_file)
                
                # ä»…å½“txtæ–‡ä»¶å­˜åœ¨æ—¶æ‰å¤„ç†
                if not os.path.exists(txt_path):
                    txt_path = None
                    
                self._generate_srt_from_segments(txt_path, mp4_path, segments_path)
                print(f"âœ… å¤„ç†_generate_srt_from_txt_process_directoryå®Œæˆï¼š{mp4_file}")
            except Exception as e:
                print(f"âŒ å¤„ç†_generate_srt_from_txt_process_directoryå¤±è´¥ï¼š{str(e)}")
                traceback.print_exc()
            print("\n")
            

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours, remainder = divmod(seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{int(hours):02}:{int(minutes):02}:{seconds:06.3f}".replace(".", ",")


    ####################æå–è§†é¢‘ä¸­çš„æ–‡æœ¬######################
    def extract_segments_from_mp4(self, mp4_path: str) -> str:
        try:
            # åˆ¤æ–­MP4æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(mp4_path):
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æŒ‡å®šçš„MP4æ–‡ä»¶ï¼š{mp4_path}")

            # è½¬å½•è§†é¢‘å†…å®¹ï¼ˆè‡ªåŠ¨æå–éŸ³é¢‘ï¼‰TODO
            result = self.whisper_model.transcribe(
                mp4_path, 
                language="en", 
                word_timestamps=True,
                task='transcribe',
                fp16=torch.cuda.is_available()
            )
            extracted_text = result.get("text", "")
            segments = result.get("segments", [])  # æ–°å¢ï¼šè·å–åˆ†æ®µä¿¡æ¯
            print(f"\nsegment:{segments}\n")
            # æ„å»ºä¸»æ–‡æœ¬è¾“å‡ºè·¯å¾„
            base_path = os.path.splitext(mp4_path)[0]
            txt_path = f"{base_path}.txt"
            segments_path = f"{base_path}_segments.txt"  # æ–°å¢ï¼šåˆ†æ®µæ–‡ä»¶å

            # åˆ é™¤å·²å­˜åœ¨çš„åŒåæ–‡ä»¶
            for path in [txt_path, segments_path]:
                if os.path.exists(path):
                    os.remove(path)
                    print(f"å·²åˆ é™¤ç°æœ‰æ–‡ä»¶ï¼š{path}")

            # å†™å…¥ä¸»æ–‡æœ¬æ–‡ä»¶
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(extracted_text)
                print(f"âœ… ä¸»æ–‡æœ¬å·²ä¿å­˜è‡³ï¼š{txt_path}")
                print(f"text:{extracted_text}\n")

            # æ–°å¢ï¼šå†™å…¥åˆ†æ®µæ–‡æœ¬æ–‡ä»¶
            if segments:
                print(f"æ£€æµ‹åˆ°åˆ†æ®µä¿¡æ¯ï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
                with open(segments_path, 'w', encoding='utf-8') as seg_file:
                    # ä½¿ç”¨json.dump()å°†åˆ—è¡¨åºåˆ—åŒ–ä¸ºJSONæ ¼å¼å†™å…¥æ–‡ä»¶
                    json.dump(segments, seg_file, ensure_ascii=False, indent=4)
                print(f"âœ… åˆ†æ®µæ•°æ®å·²JSONåºåˆ—åŒ–ä¿å­˜è‡³ï¼š{segments_path}")

            return extracted_text
            
        except Exception as e:
            print(f"âŒ è½¬å½•å¤±è´¥ï¼š{str(e)}")
            raise RuntimeError(f"è§†é¢‘è½¬å½•å¤±è´¥: {str(e)}") from e

    ####################å°†å­—å¹•ä¸­çš„çŸ­å¥åˆå¹¶ä¸ºé•¿å¥######################

    def merge_srt(self, input_srt_file: str, output_srt_file: str = None) -> None:
        """å¤„ç†SRTæ–‡ä»¶åˆå¹¶å¹¶ä¿å­˜"""
        # è¯»å–è¾“å…¥æ–‡ä»¶
        subs = pysrt.open(input_srt_file,"utf-8")
        
        # è°ƒç”¨åŸæœ‰åˆå¹¶é€»è¾‘
        merged_subs = self._merge_srt(subs)
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        save_path = output_srt_file if output_srt_file else input_srt_file
        

        # å†™å…¥å¤„ç†ç»“æœ
        merged_subs.save(save_path, encoding='utf-8')
        
        print(f"å¤„ç†å®Œæˆï¼Œä¿å­˜è·¯å¾„ï¼š{save_path}")


    def _merge_srt(self, subs: SubRipFile,max_merge_lines=999) -> SubRipFile:
        merged_subs = []
        i = 0
        
        while i < len(subs):
            current_group = [subs[i]]
            merged_text = subs[i].text
            next_candidate = i + 1
            
            print(f"\nå¼€å§‹å¤„ç†å­—å¹•ç»„ï¼Œèµ·å§‹ç´¢å¼•: {i}")
            print(f"åˆå§‹åˆå¹¶ç»„: {[sub.text for sub in current_group]}")
            print(f"åˆå§‹åˆå¹¶æ–‡æœ¬: '{merged_text}'")

            while True:
                if next_candidate >= len(subs) or len(current_group) >= max_merge_lines:
                    print(f"ç»ˆæ­¢åˆå¹¶ - å‰©ä½™å­—å¹•: {len(subs)-next_candidate}, å½“å‰åˆå¹¶è¡Œæ•°: {len(current_group)}")
                    break
                
                next_sub = subs[next_candidate]
                next_next_sub = subs[next_candidate + 1] if next_candidate + 1 < len(subs) else None
                
                try:
                    args = [merged_text, next_sub.text]
                    if next_next_sub:
                        args.append(next_next_sub.text)
                    
                    # æ‰“å°åˆå¹¶åˆ¤æ–­å‚æ•°
                    print(f"\nå°è¯•åˆå¹¶ {len(current_group)} è¡Œåˆ° {len(current_group)+1} è¡Œ")
                    print(f"å½“å‰åˆå¹¶æ–‡æœ¬: '{args[0]}'")
                    print(f"ä¸‹ä¸€è¡Œæ–‡æœ¬:  '{args[1]}'")
                    print(f"ç¬¬ä¸‰è¡Œå­˜åœ¨:   {args[2] if len(args)>2 else 'æ— '}")

                    if self._should_merge(*args):
                        current_group.append(next_sub)
                        merged_text = self._format_text(current_group)
                        next_candidate += 1
                        print(f"âœ… åˆå¹¶æˆåŠŸï¼å½“å‰åˆå¹¶è¡Œæ•°: {len(current_group)}")
                        print(f"æ›´æ–°ååˆå¹¶æ–‡æœ¬: '{merged_text}'")
                    else:
                        print(f"âŒ åˆå¹¶ç»ˆæ­¢ {len(current_group)} è¡Œ")
                        break
                except LanguageMismatchError as e:
                    print(f"ğŸš« è¯­è¨€ä¸åŒ¹é…å¼‚å¸¸: {str(e)}")
                    break

            new_sub = self._create_merged_sub(current_group, merged_text)
            merged_subs.append(new_sub)
            print(f"\nç”Ÿæˆåˆå¹¶æ¡ç›®: {new_sub.start} -> {new_sub.end}")
            print(f"æœ€ç»ˆåˆå¹¶æ–‡æœ¬: '{new_sub.text}'\n")
            
            i = next_candidate
        
        print(f"\nåˆå¹¶å®Œæˆï¼Œæ€»æ¡ç›®æ•°: {len(merged_subs)}")
        return SubRipFile(items=merged_subs)
        
    def _format_text(self, group):
        return format_merged_text([sub.text for sub in group])
    
    def _create_merged_sub(self, group, text):
        return SubRipItem(
            start=group[0].start,
            end=group[-1].end,
            text=text
        )
    
    def _should_merge(self, text1: str, text2: str, text3: str=None) -> bool:
        print(f"\næ‰§è¡Œåˆå¹¶åˆ¤æ–­ _should_merge:")
        print(f"text1 ({len(text1)} chars): '{text1}'")
        print(f"text2 ({len(text2)} chars): '{text2}'")
        print(f"text3 ({len(text3) if text3 else 'None'}): '{text3 if text3 else 'æ— '}'")

        if detect_language(text1) != detect_language(text2):
            print("è¯­è¨€ä¸åŒ¹é…è§¦å‘å¼‚å¸¸")
            raise LanguageMismatchError("Language mismatch")

        # æ‰“å°è¯­è¨€æ£€æµ‹ç»“æœ
        lang1 = detect_language(text1)
        lang2 = detect_language(text2)
        lang3 = detect_language(text3) if text3 else None
        print(f"è¯­è¨€æ£€æµ‹ç»“æœ: text1={lang1}, text2={lang2}, text3={lang3}")

        if lang1 != lang2 or lang2 != lang3:
            print(f"è¯­è¨€ä¸åŒ¹é…: {lang1} != {lang2} != {lang3} text1={text1} text2={text2} text3={text3}")
            return False

        prompt = ""
        if text3 is not None and lang1 == lang2 and lang2 == lang3:
            prompt = f"åœ¨ä¸€ä¸ªæ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„å­—å¹•æ–‡ä»¶ä¸­ï¼Œç¬¬ä¸‰è¡Œå­—å¹•æ˜¯â€œ{text3}â€çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€è¡Œå­—å¹•å’Œç¬¬äºŒè¡Œæ˜¯å¦å¯èƒ½æ˜¯å±äºåŒä¸€å¥è¯?å¯èƒ½å±äºçš„å›ç­”æˆ‘â€œYâ€ï¼Œå¯èƒ½ä¸å±äºå›ç­”æˆ‘â€œNâ€ï¼Œä¸è¦å›ç­”ä»»ä½•ä¿¡æ¯ã€‚\nç¬¬ä¸€è¡Œ: {text1}\nç¬¬äºŒè¡Œ: {text2}"
        else:
            prompt = f"åœ¨ä¸€ä¸ªæ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„å­—å¹•æ–‡ä»¶ä¸­ï¼Œåœ¨æ²¡æœ‰ç¬¬ä¸‰è¡Œå­—å¹•çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€è¡Œå­—å¹•å’Œç¬¬äºŒè¡Œæ˜¯å¦å¯èƒ½æ˜¯å±äºåŒä¸€å¥è¯?å¯èƒ½å±äºçš„å›ç­”æˆ‘â€œYâ€ï¼Œå¯èƒ½ä¸å±äºå›ç­”æˆ‘â€œNâ€ï¼Œä¸è¦å›ç­”ä»»ä½•ä¿¡æ¯ã€‚\nç¬¬ä¸€è¡Œ: {text1}\nç¬¬äºŒè¡Œ: {text2}"
        
        print(f"å‘é€ç»™LLMçš„æç¤º:\n{prompt}")
        response = call_llm_api(prompt, api_key=self.api_key)
        print(f"LLMå“åº”åŸå§‹ç»“æœ: '{response}'")
        
        result = 'Y' in response
        print(f"åˆå¹¶åˆ¤æ–­æœ€ç»ˆç»“æœ: {'Y' if result else 'N'}")
        return result


    ####################ç¿»è¯‘######################
    def add_translation(self, input_srt_file: str, output_srt_file: str = None, 
                   target_lang: str = "zh") -> None:
        """æ·»åŠ åŒè¯­å­—å¹•å¹¶ä¿å­˜SRTæ–‡ä»¶
        :param target_lang: ç›®æ ‡è¯­è¨€ä»£ç ï¼ˆé»˜è®¤ä¸­æ–‡ï¼‰
        """
        input_srt_file = os.path.abspath(input_srt_file)
        # åˆ¤è¯»åç¼€æ˜¯å¦æ­£ç¡®
        if not input_srt_file.endswith('.srt'):
            raise ValueError("è¾“å…¥æ–‡ä»¶å¿…é¡»ä¸ºSRTæ ¼å¼")
        subs = pysrt.open(input_srt_file, encoding=detect_encoding(input_srt_file))
        processed_subs = self._translate_subs(subs, target_lang)
        save_path = output_srt_file if output_srt_file else input_srt_file
        processed_subs.save(save_path, encoding='utf-8')
        print(f"åŒè¯­å­—å¹•å·²ç”Ÿæˆï¼Œä¿å­˜è·¯å¾„ï¼š{save_path}")

    def _translate_subs(self, subs: SubRipFile, target_lang: str) -> SubRipFile:
        for sub in subs:
            src_text = sub.text.strip()
            
            # è¯­è¨€æ£€æµ‹ä¸è¿‡æ»¤ï¼ˆç¤ºä¾‹ä»…å¤„ç†è‹±æ–‡ç¿»è¯‘ï¼‰
            if detect_language(src_text) not in ['en']:
                continue  # è·³è¿‡éè‹±æ–‡å†…å®¹
            
            # æ„é€ ç¿»è¯‘æŒ‡ä»¤ï¼ˆæ”¯æŒæ ¼å¼æ§åˆ¶ï¼‰
            prompt = f"""å°†ä»¥ä¸‹å­—å¹•ç²¾ç¡®ç¿»è¯‘ä¸º{target_lang}ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­ï¼Œç¦æ­¢æ·»åŠ å…¶ä»–å†…å®¹ï¼Œä¸“ä¸šæœ¯è¯­å¯ä»¥ç›´æ¥ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œå¦‚æœæ˜¯ç–‘é—®å¥å¯ä»¥é€‚å½“æ·»åŠ è¯­æ°”åŠ©è¯ã€‚ç›´æ¥å›å¤ç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦å›ç­”å…¶ä»–æ— å…³è¯è¯­ã€‚
            {src_text}"""
            print(f"æé—®ï¼š{prompt}")
            # è°ƒç”¨ç¿»è¯‘APIï¼ˆæ”¯æŒå¤±è´¥é‡è¯•æœºåˆ¶ï¼‰
            try:
                translated = call_llm_api(
                    prompt,
                    api_key=self.api_key,
                    model="qwen-max",
                    temperature=0.2  # é™ä½éšæœºæ€§
                )
                
                print(f"å›ç­”ï¼š{translated.strip()}\n")
                # æ¸…æ´—APIè¿”å›ç»“æœ
                sub.text = f"{src_text}\n{translated.strip()}"
            except Exception as e:
                print(f"ç¿»è¯‘å¤±è´¥ï¼š{str(e)}ï¼Œä½¿ç”¨whisperç¿»è¯‘")
                # ä½¿ç”¨whisperç¿»è¯‘
                translated = self.whisper_model.translate(src_text, target_lang=target_lang)
                print(f"whisperå›ç­”ï¼š{translated.strip()}\n")
                sub.text = f"{src_text}\n{translated.strip()}"
        return subs

    ####################ä¿®æ”¹å­—å¹•ä¸­çš„å•è¯çš„é”™è¯¯æ‹¼å†™######################
    def correct_spelling(self, input_srt_file: str, output_srt_file: str = None) -> None:
        """å¤„ç†SRTæ–‡ä»¶åˆå¹¶å¹¶ä¿å­˜"""
        # è¯»å–è¾“å…¥æ–‡ä»¶
        subs = pysrt.open(input_srt_file, encoding='utf-8')
        
        # è°ƒç”¨åŸæœ‰åˆå¹¶é€»è¾‘
        merged_subs = self._correct_spelling(subs)
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        save_path = output_srt_file if output_srt_file else input_srt_file
        
        # å†™å…¥å¤„ç†ç»“æœ
        merged_subs.save(save_path, encoding='utf-8')
        
        print(f"å¤„ç†å®Œæˆï¼Œä¿å­˜è·¯å¾„ï¼š{save_path}")
    
    
    def _correct_spelling(self, subs: SubRipFile) -> SubRipFile:
        for sub in subs:
            
            # ç”Ÿæˆæ‹¼å†™ä¿®æ­£æŒ‡ä»¤
            prompt = f"""è¯·ä¿®æ­£ä»¥ä¸‹å­—å¹•å†…å®¹çš„å•è¯æ‹¼å†™é”™è¯¯ï¼Œå›ç­”ä¿®æ­£åçš„å­—å¹•å†…å®¹ï¼Œä¸è¦è¿”å›å…¶ä»–ä»»ä½•æ— å…³çš„å†…å®¹ï¼š
            {sub.text}"""
            print(f"é—®é¢˜ï¼š{prompt}")
            # è°ƒç”¨LLM APIï¼ˆæ”¯æŒå¤šå¹³å°è°ƒç”¨ï¼‰
            corrected = call_llm_api(
                prompt, 
                api_key=self.api_key,
                model="qwen-max"  # æ¨èä½¿ç”¨æœ€æ–°æ¨¡å‹
            )
            sub.text = corrected.strip()  # å–æœ€åä¸€è¡Œé˜²æ­¢é™„åŠ è¯´æ˜
            print(f"å›ç­”ï¼š{corrected.strip()}\n")
        return subs

    def _extract_audio_segment(self, audio, start_sec: float, end_sec: float):
        """éŸ³é¢‘åˆ‡å‰²å·¥å…·ï¼ˆéœ€å®‰è£…pydubï¼‰"""
        return audio[start_sec*1000:end_sec*1000]

    def correct_and_save(self, srt_path: str, audio_path: str):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        subs = pysrt.open(srt_path, encoding='utf-8')
        corrected = self.correct_spelling(subs, audio_path)
        corrected.save(srt_path, encoding='utf-8')
        print(f"å·²å®Œæˆæ‹¼å†™ä¿®æ­£å¹¶è¦†ç›–ä¿å­˜ï¼š{srt_path}")


    ####################å°†å­—å¹•ä¸­çš„é•¿å¥æ‰åˆ†ä¸ºçŸ­å¥(æœ‰bugä¸å¯ç”¨)######################


    def split_long_lines(self, input_srt_file: str, output_srt_file: str = None, max_line_count=99) -> None:
        """
        ä¼˜åŒ–å­—å¹•æ–‡ä»¶ï¼šæ‹†åˆ†è¶…é•¿è¡Œå¹¶é‡æ–°è®¡ç®—æ—¶é—´è½´
        :param input_srt_file: è¾“å…¥SRTæ–‡ä»¶è·¯å¾„
        :param output_srt_file: è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„ï¼ˆç©ºåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
        """
        subs = pysrt.open(input_srt_file, encoding='utf-8')
        mp4file = input_srt_file.replace('.srt', '.mp4')
        if not os.path.exists(mp4file):
            raise FileNotFoundError("æœªæ‰¾åˆ°å¯¹åº”çš„è§†é¢‘æ–‡ä»¶"+mp4file)
        
        # åŠ è½½è§†é¢‘å¹¶æå–éŸ³é¢‘
        audio = AudioSegment.from_file(mp4file, format="mp4")

        new_subs = SubRipFile()
        for idx, sub in enumerate(subs):
            if len(sub.text) <= max_line_count:
                new_subs.append(sub)
                continue
            
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œè¯­ä¹‰æ‹†åˆ†â€Œ:ml-citation{ref="2" data="citationList"}
            prompt = f"å°†ä»¥ä¸‹ä¸€è¡Œé•¿å­—å¹•å†…å®¹æŒ‰è‹±æ–‡è¯­ä¹‰æ‹†åˆ†ä¸ºå¤šè¡Œå­—å¹•æ˜¾ç¤ºï¼Œä¿æŒåŸå¥å†…å®¹çš„ä¸å˜ï¼Œä¸è¦è¿”å›è¯­å¥å¤–çš„å…¶ä»–ä»»ä½•å¤šä½™çš„å†…å®¹ï¼Œå°†æ‹†åˆ†å‡ºæ¥çš„å¤šè¡Œå­—å¹•çš„æ¯ä¸€è¡Œç”¨æ¢è¡Œéš”å¼€ï¼š\n{sub.text}"
            print(f"æé—®ï¼š{prompt}")
            response = call_llm_api(prompt, self.api_key, model="qwen-max")
            split_lines = [line.strip() for line in response.split('\n') if line.strip()]
            print(f"å›ç­”ï¼š{"\n".join(split_lines)}\n")
            # åˆ†å‰²æ—¶é—´æ®µå¹¶ç”Ÿæˆæ–°å­—å¹•é¡¹â€Œ:ml-citation{ref="1" data="citationList"}
            temp_mp3 = os.path.join(os.path.dirname(input_srt_file), "temp_clip.mp3")
            try:
                time_segments = self._split_time_segment(sub, split_lines, audio, temp_mp3)
            finally:
                if os.path.exists(temp_mp3):
                    os.remove(temp_mp3)
            print(f"\n  split_lines_len:{len(split_lines)}, time_segments_len:{len(time_segments)}\n")
            for i, (text, (start, end)) in enumerate(zip(split_lines, time_segments)):
                new_item = SubRipItem(
                    index=len(new_subs)+1,
                    start=pysrt.SubRipTime(milliseconds=start),
                    end=pysrt.SubRipTime(milliseconds=end),
                    text=text
                )
                print(f"new_item:{new_item}")
                new_subs.append(new_item)
        
        save_path = output_srt_file or input_srt_file
        new_subs.save(save_path, encoding='utf-8')
    
    def _split_time_segment(self, sub: SubRipItem, split: List[str], audio,temp_mp3) -> List[tuple]:
        """
        ä½¿ç”¨Whisperç²¾ç¡®åˆ†å‰²æ—¶é—´æ®µâ€Œ:ml-citation{ref="1" data="citationList"}
        """
        split_count = len(split)
        start_ms = sub.start.ordinal
        end_ms = sub.end.ordinal
        segment_duration = (end_ms - start_ms) / split_count
        if split_count > 2:
            if os.path.exists(temp_mp3):
                os.remove(temp_mp3)
            clip = audio[start_ms:end_ms]  # start_msç§’åˆ°end_msç§’ï¼ˆå•ä½ï¼šæ¯«ç§’ï¼‰
            clip.export(temp_mp3, format="mp3")

            # åˆ¤æ–­å½“å‰è®¾å¤‡æ˜¯å¦ä¸º GPU
            use_gpu = torch.cuda.is_available()
            # è°ƒç”¨Whisperæ¨¡å‹
            result = self.whisper_model.transcribe(
                temp_mp3, 
                language="en",
                word_timestamps=True,
                task='transcribe',  # æ˜ç¡®ä»»åŠ¡ç±»å‹
                fp16=use_gpu  # GPU å¯ç”¨ FP16ï¼ŒCPU ç¦ç”¨
            )
            return self._align_whisper_timestamps(result, split)
        
        # ç®€å•å¹³å‡åˆ†é…æ—¶é—´
        return [(start_ms + i*segment_duration, 
                start_ms + (i+1)*segment_duration) 
                for i in range(split_count)]


    def _align_whisper_timestamps(self, whisper_result, split_list: List[str]) -> List[tuple]:
        """
        åŸºäºåŠ¨æ€æ—¶é—´è§„æ•´çš„ç²¾ç»†åŒ–æ—¶é—´æˆ³å¯¹é½
        """
        # æå–è¯†åˆ«æ–‡æœ¬ä¸æ—¶é—´æˆ³åºåˆ—
        recognized_words = [word['word'].lower() for segment in whisper_result['segments'] 
                        for word in segment['words']]
        word_timestamps = [{'start': word['start'], 'end': word['end']} 
                        for segment in whisper_result['segments'] 
                        for word in segment['words']]

        # æ„å»ºç›®æ ‡æ–‡æœ¬åºåˆ—ï¼ˆå¤„ç†æ ‡ç‚¹ç¬¦å·ï¼‰
        target_words = []
        for line in split_list:
            target_words.extend(re.findall(r"\w+", line.lower()))

        # ä½¿ç”¨åŠ¨æ€æ—¶é—´è§„æ•´(DTW)å¯¹é½
        alignment_path = self._dtw_alignment(recognized_words, target_words)

        # æŒ‰åˆ†å‰²ç‚¹åˆ†é…æ—¶é—´æˆ³
        split_indices = self._find_split_indices(alignment_path, len(split_list))
        
        # åˆå¹¶å¯¹åº”æ—¶é—´èŒƒå›´
        time_ranges = []
        for i in range(len(split_indices)-1):
            start_idx = split_indices[i]
            end_idx = split_indices[i+1]-1
            start_time = word_timestamps[start_idx]['start'] * 1000  # è½¬æ¯«ç§’
            if end_idx < len(word_timestamps):
                end_time = word_timestamps[end_idx]['end'] * 1000
            else:
                end_time = word_timestamps[-1]['end'] * 1000  # å–æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æˆ³
                print(f"Split index {end_idx} exceeds timestamp array length")
            time_ranges.append((start_time, end_time))
        
        return time_ranges

    def _dtw_alignment(self, src_seq, tgt_seq):
        """
        åŠ¨æ€æ—¶é—´è§„æ•´å¯¹é½ç®—æ³•å®ç°
        """
        # åˆ›å»ºè·ç¦»çŸ©é˜µï¼ˆæ­¤å¤„ä½¿ç”¨Levenshteinè·ç¦»ï¼‰
        distance_matrix = np.zeros((len(src_seq)+1, len(tgt_seq)+1))
        for i in range(len(src_seq)+1):
            for j in range(len(tgt_seq)+1):
                if i == 0 or j == 0:
                    distance_matrix[i][j] = max(i, j)
                else:
                    cost = 0 if src_seq[i-1] == tgt_seq[j-1] else 1
                    distance_matrix[i][j] = cost + min(
                        distance_matrix[i-1][j],    # æ’å…¥
                        distance_matrix[i][j-1],    # åˆ é™¤ 
                        distance_matrix[i-1][j-1]   # æ›¿æ¢
                    )

        # å›æº¯å¯»æ‰¾æœ€ä¼˜è·¯å¾„
        i, j = len(src_seq), len(tgt_seq)
        path = [(i, j)]
        while i > 0 and j > 0:
            min_dir = np.argmin([
                distance_matrix[i-1][j], 
                distance_matrix[i][j-1],
                distance_matrix[i-1][j-1]
            ])
            if min_dir == 0:
                i -= 1
            elif min_dir == 1:
                j -= 1
            else:
                i -= 1
                j -= 1
            path.append((i, j))
        
        return list(reversed(path))
    
    def _find_split_indices(self, alignment_path: List[tuple], num_splits: int) -> List[int]:
        """åŠ¨æ€åˆ†å‰²ç‚¹æ£€æµ‹ç®—æ³•"""
        # åˆå§‹åŒ–åˆ†å‰²ç‚¹å®¹å™¨ï¼ˆåŒ…å«èµ·å§‹ç‚¹0ï¼‰
        split_points = [0]
        current_split = 0
        target_step = len(alignment_path) // num_splits
        
        # é˜¶æ®µ1ï¼šåŸºäºDTWè·¯å¾„çš„åˆæ­¥åˆ†å‰²
        for path_idx, (src_idx, tgt_idx) in enumerate(alignment_path):
            if tgt_idx > current_split:
                # è®°å½•å½“å‰åˆ†å‰²ç‚¹å¯¹åº”çš„æºåºåˆ—ä½ç½®
                split_points.append(src_idx)
                current_split += 1
                
                # æå‰è¾¾åˆ°åˆ†å‰²æ•°æ—¶é€€å‡º
                if current_split >= num_splits:
                    break
        
        # é˜¶æ®µ2ï¼šå‡åŒ€å¡«å……å‰©ä½™åˆ†å‰²ç‚¹ï¼ˆå¤„ç†ä¸è¶³æƒ…å†µï¼‰
        while len(split_points) < num_splits + 1:
            last_point = split_points[-1]
            remaining = len(alignment_path) - last_point
            step = max(1, remaining // (num_splits - len(split_points) + 1))
            split_points.append(last_point + step)
        
        # é˜¶æ®µ3ï¼šåå¤„ç†ä¼˜åŒ–ï¼ˆç¡®ä¿å•è°ƒé€’å¢ï¼‰
        processed = [split_points[0]]
        for p in split_points[1:]:
            if p <= processed[-1]:
                processed.append(processed[-1] + 1)
            else:
                processed.append(p)
        
        # æˆªæ–­è‡³å®é™…è·¯å¾„é•¿åº¦
        processed[-1] = len(alignment_path) - 1
        return sorted(list(set(processed)))[:num_splits+1]